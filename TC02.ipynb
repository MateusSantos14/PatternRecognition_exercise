{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "209c77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc650f",
   "metadata": {},
   "source": [
    "# Métodos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54bcdc8",
   "metadata": {},
   "source": [
    "## DMC(Distância Minima do Centroide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a39295e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMC(data, Nr, Ptrain):\n",
    "    \"\"\"\n",
    "    Implementa o classificador de Distância Mínima do Centroide (DMC).\n",
    "\n",
    "    Parâmetros:\n",
    "    - data: Array numpy com os dados (features nas primeiras colunas, rótulo na última).\n",
    "    - Nr: Número de execuções (rodadas) para avaliação do classificador.\n",
    "    - Ptrain: Percentual de dados a serem usados para treinamento.\n",
    "\n",
    "    Retorna:\n",
    "    - STATS: Estatísticas de acurácia (média, min, max, mediana, desvio padrão).\n",
    "    - TX_OK: Vetor com a taxa de acerto de cada uma das Nr execuções.\n",
    "    - X: Dicionário com as amostras de treino por classe (da última execução).\n",
    "    - m: Dicionário com os vetores de média (centroides) por classe (da última execução).\n",
    "    \"\"\"\n",
    "    n_total_samples, p_plus_1 = data.shape\n",
    "    num_features = p_plus_1 - 1\n",
    "    X_data = data[:, :-1]\n",
    "    y_data = data[:, -1].astype(int)\n",
    "\n",
    "    Ntrn = round(Ptrain * n_total_samples / 100)\n",
    "    Ntst = n_total_samples - Ntrn\n",
    "\n",
    "    # Encontra o número de classes. Supõe que as classes são rotuladas de 1 a K.\n",
    "    unique_labels = np.unique(y_data)\n",
    "    K = len(unique_labels)\n",
    "    print(f'O problema tem {K} classes, {num_features} features. Método DMC.')\n",
    "\n",
    "    TX_OK = np.zeros(Nr)\n",
    "\n",
    "    for r in range(Nr):\n",
    "        # Embaralhamento e divisão dos dados\n",
    "        permutation = np.random.permutation(n_total_samples)\n",
    "        train_indices = permutation[:Ntrn]\n",
    "        test_indices = permutation[Ntrn:]\n",
    "\n",
    "        X_train, y_train = X_data[train_indices], y_data[train_indices]\n",
    "        X_test, y_test = X_data[test_indices], y_data[test_indices]\n",
    "\n",
    "        # Dicionários para armazenar os resultados da última execução\n",
    "        X, m = {}, {}\n",
    "        \n",
    "        # Treinamento: Cálculo dos centroides para cada classe\n",
    "        classes_in_train = np.unique(y_train)\n",
    "        for k in classes_in_train:\n",
    "            X_k = X_train[y_train == k]\n",
    "            X[k] = X_k\n",
    "            m[k] = np.mean(X_k, axis=0)\n",
    "\n",
    "        # Teste: Classificação das amostras\n",
    "        # Matriz para armazenar as distâncias de cada amostra de teste para cada centroide\n",
    "        distances = np.zeros((Ntst, len(classes_in_train)))\n",
    "\n",
    "        for i, k in enumerate(classes_in_train):\n",
    "            # Calcula a distância Euclidiana de todas as amostras de teste para o centroide da classe k\n",
    "            distances[:, i] = np.linalg.norm(X_test - m[k], axis=1)\n",
    "        \n",
    "        # A classe predita é aquela com a menor distância (argmin)\n",
    "        predicted_class_indices = np.argmin(distances, axis=1)\n",
    "        predicted_labels = classes_in_train[predicted_class_indices]\n",
    "\n",
    "        # Cálculo da acurácia para a rodada atual\n",
    "        correct = np.sum(predicted_labels == y_test)\n",
    "        TX_OK[r] = 100 * correct / Ntst\n",
    "\n",
    "    # Consolidação dos resultados\n",
    "    STATS = np.array([\n",
    "        np.mean(TX_OK), np.min(TX_OK), np.max(TX_OK), np.median(TX_OK), np.std(TX_OK)\n",
    "    ])\n",
    "\n",
    "    return STATS, TX_OK, X, m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9688c1",
   "metadata": {},
   "source": [
    "## 1-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9146406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN1(data, Nr, Ptrain):\n",
    "    \"\"\"\n",
    "    Implementa o classificador do Vizinho Mais Próximo (1-NN).\n",
    "\n",
    "    Parâmetros:\n",
    "    - data: Array numpy com os dados (features nas primeiras colunas, rótulo na última).\n",
    "    - Nr: Número de execuções (rodadas) para avaliação do classificador.\n",
    "    - Ptrain: Percentual de dados a serem usados para treinamento.\n",
    "\n",
    "    Retorna:\n",
    "    - STATS: Estatísticas de acurácia (média, min, max, mediana, desvio padrão).\n",
    "    - TX_OK: Vetor com a taxa de acerto de cada uma das Nr execuções.\n",
    "    - X_train_last_run: Amostras de treino da última execução.\n",
    "    - y_train_last_run: Rótulos de treino da última execução.\n",
    "    \"\"\"\n",
    "    n_total_samples, p_plus_1 = data.shape\n",
    "    num_features = p_plus_1 - 1\n",
    "    X_data = data[:, :-1]\n",
    "    y_data = data[:, -1].astype(int)\n",
    "\n",
    "    Ntrn = round(Ptrain * n_total_samples / 100)\n",
    "    Ntst = n_total_samples - Ntrn\n",
    "\n",
    "    K = len(np.unique(y_data))\n",
    "    print(f'O problema tem {K} classes, {num_features} features. Método 1-NN.')\n",
    "\n",
    "    TX_OK = np.zeros(Nr)\n",
    "\n",
    "    # Variáveis para guardar os dados da última execução\n",
    "    X_train_last_run, y_train_last_run = None, None\n",
    "\n",
    "    for r in range(Nr):\n",
    "        permutation = np.random.permutation(n_total_samples)\n",
    "        train_indices = permutation[:Ntrn]\n",
    "        test_indices = permutation[Ntrn:]\n",
    "\n",
    "        X_train, y_train = X_data[train_indices], y_data[train_indices]\n",
    "        X_test, y_test = X_data[test_indices], y_data[test_indices]\n",
    "        \n",
    "        if r == Nr - 1:\n",
    "            X_train_last_run = X_train\n",
    "            y_train_last_run = y_train\n",
    "\n",
    "        # Treinamento: Apenas memoriza os dados (não há cálculo de modelo)\n",
    "\n",
    "        # Teste: Classificação das amostras\n",
    "        predicted_labels = np.zeros(Ntst, dtype=int)\n",
    "        \n",
    "        for i in range(Ntst):\n",
    "            # Calcula a distância da amostra de teste atual para TODAS as amostras de treino\n",
    "            distances = np.linalg.norm(X_train - X_test[i, :], axis=1)\n",
    "            \n",
    "            # Encontra o índice da amostra de treino com a menor distância\n",
    "            nearest_neighbor_index = np.argmin(distances)\n",
    "            \n",
    "            # Atribui o rótulo do vizinho mais próximo\n",
    "            predicted_labels[i] = y_train[nearest_neighbor_index]\n",
    "\n",
    "        correct = np.sum(predicted_labels == y_test)\n",
    "        TX_OK[r] = 100 * correct / Ntst\n",
    "\n",
    "    # Consolidação dos resultados\n",
    "    STATS = np.array([\n",
    "        np.mean(TX_OK), np.min(TX_OK), np.max(TX_OK), np.median(TX_OK), np.std(TX_OK)\n",
    "    ])\n",
    "\n",
    "    return STATS, TX_OK, X_train_last_run, y_train_last_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2cec8",
   "metadata": {},
   "source": [
    "## Máxima correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e98b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxCorr(data, Nr, Ptrain):\n",
    "    \"\"\"\n",
    "    Implementa o Classificador de Máxima Correlação.\n",
    "\n",
    "    Parâmetros:\n",
    "    - data: Array numpy com os dados (features nas primeiras colunas, rótulo na última).\n",
    "    - Nr: Número de execuções (rodadas) para avaliação do classificador.\n",
    "    - Ptrain: Percentual de dados a serem usados para treinamento.\n",
    "\n",
    "    Retorna:\n",
    "    - STATS: Estatísticas de acurácia (média, min, max, mediana, desvio padrão).\n",
    "    - TX_OK: Vetor com a taxa de acerto de cada uma das Nr execuções.\n",
    "    - X: Dicionário com as amostras de treino por classe (da última execução).\n",
    "    - m: Dicionário com os vetores de média (centroides) por classe (da última execução).\n",
    "    \"\"\"\n",
    "    n_total_samples, p_plus_1 = data.shape\n",
    "    num_features = p_plus_1 - 1\n",
    "    X_data = data[:, :-1]\n",
    "    y_data = data[:, -1].astype(int)\n",
    "\n",
    "    Ntrn = round(Ptrain * n_total_samples / 100)\n",
    "    Ntst = n_total_samples - Ntrn\n",
    "\n",
    "    unique_labels = np.unique(y_data)\n",
    "    K = len(unique_labels)\n",
    "    print(f'O problema tem {K} classes, {num_features} features. Método Máxima Correlação.')\n",
    "\n",
    "    TX_OK = np.zeros(Nr)\n",
    "\n",
    "    for r in range(Nr):\n",
    "        permutation = np.random.permutation(n_total_samples)\n",
    "        train_indices = permutation[:Ntrn]\n",
    "        test_indices = permutation[Ntrn:]\n",
    "\n",
    "        X_train, y_train = X_data[train_indices], y_data[train_indices]\n",
    "        X_test, y_test = X_data[test_indices], y_data[test_indices]\n",
    "\n",
    "        X, m, m_norm = {}, {}, {}\n",
    "        \n",
    "        # Treinamento: Cálculo dos centroides e sua normalização\n",
    "        classes_in_train = np.unique(y_train)\n",
    "        for k in classes_in_train:\n",
    "            X_k = X_train[y_train == k]\n",
    "            X[k] = X_k\n",
    "            \n",
    "            # Calcula o centroide\n",
    "            centroid = np.mean(X_k, axis=0)\n",
    "            m[k] = centroid\n",
    "            \n",
    "            # Normaliza o centroide (adiciona epsilon para evitar divisão por zero)\n",
    "            norm_centroid = np.linalg.norm(centroid)\n",
    "            m_norm[k] = centroid / (norm_centroid + 1e-9)\n",
    "\n",
    "        # Teste: Classificação das amostras\n",
    "        \n",
    "        # Normaliza todas as amostras de teste (adiciona epsilon para evitar divisão por zero)\n",
    "        # O keepdims=True garante que a divisão seja feita linha por linha corretamente\n",
    "        norms_X_test = np.linalg.norm(X_test, axis=1, keepdims=True)\n",
    "        X_test_norm = X_test / (norms_X_test + 1e-9)\n",
    "        \n",
    "        # Matriz para armazenar as correlações (produtos escalares)\n",
    "        correlations = np.zeros((Ntst, len(classes_in_train)))\n",
    "\n",
    "        for i, k in enumerate(classes_in_train):\n",
    "            # Calcula o produto escalar entre todos os vetores de teste normalizados e o centroide normalizado da classe k\n",
    "            correlations[:, i] = X_test_norm @ m_norm[k]\n",
    "        \n",
    "        # A classe predita é aquela com a maior correlação (argmax)\n",
    "        predicted_class_indices = np.argmax(correlations, axis=1)\n",
    "        predicted_labels = classes_in_train[predicted_class_indices]\n",
    "\n",
    "        correct = np.sum(predicted_labels == y_test)\n",
    "        TX_OK[r] = 100 * correct / Ntst\n",
    "\n",
    "    # Consolidação dos resultados\n",
    "    STATS = np.array([\n",
    "        np.mean(TX_OK), np.min(TX_OK), np.max(TX_OK), np.median(TX_OK), np.std(TX_OK)\n",
    "    ])\n",
    "\n",
    "    return STATS, TX_OK, X, m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd62cc9",
   "metadata": {},
   "source": [
    "## Quadrático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "667254c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratico(data, Nr, Ptrain):\n",
    "    \"\"\"\n",
    "    Implementa o classificador de Análise Discriminante Quadrática (QDA).\n",
    "\n",
    "    Parâmetros:\n",
    "    - data: Array numpy com os dados (features na primeiras colunas, rótulo na última).\n",
    "    - Nr: Número de execuções (rodadas) para avaliação do classificador.\n",
    "    - Ptrain: Percentual de dados a serem usados para treinamento.\n",
    "\n",
    "    Retorna:\n",
    "    - STATS: Estatísticas de acurácia (média, min, max, mediana, desvio padrão).\n",
    "    - TX_OK: Vetor com a taxa de acerto de cada uma das Nr execuções.\n",
    "    - X: Dicionário com as amostras de treino por classe (da última execução).\n",
    "    - m: Dicionário com os vetores de média por classe (da última execução).\n",
    "    - S: Dicionário com as matrizes de covariância por classe (da última execução).\n",
    "    - posto: Dicionário com os ranks das matrizes de covariância (da última execução).\n",
    "    \"\"\"\n",
    "    n_total_samples, p_plus_1 = data.shape\n",
    "    num_features = p_plus_1 - 1\n",
    "    X_data = data[:, :-1]\n",
    "    y_data = data[:, -1].astype(int)\n",
    "\n",
    "    Ntrn = round(Ptrain * n_total_samples / 100)\n",
    "    Ntst = n_total_samples - Ntrn\n",
    "\n",
    "    K = np.max(y_data)\n",
    "    print(f'O problema tem {K} classes, {num_features} features. Método QDA.')\n",
    "\n",
    "    TX_OK = np.zeros(Nr)\n",
    "\n",
    "    for r in range(Nr):\n",
    "        permutation = np.random.permutation(n_total_samples)\n",
    "        train_indices = permutation[:Ntrn]\n",
    "        test_indices = permutation[Ntrn:]\n",
    "\n",
    "        X_train, y_train = X_data[train_indices], y_data[train_indices]\n",
    "        X_test, y_test = X_data[test_indices], y_data[test_indices]\n",
    "\n",
    "        X, m, S, posto = {}, {}, {}, {}\n",
    "        inv_covs, log_dets, log_priors = {}, {}, {}\n",
    "\n",
    "        unique_classes_in_train = np.unique(y_train)\n",
    "\n",
    "        # Treinamento: Cálculo dos parâmetros para cada classe\n",
    "        for k in unique_classes_in_train:\n",
    "            X_k = X_train[y_train == k]\n",
    "            X[k] = X_k\n",
    "            \n",
    "            m_k = np.mean(X_k, axis=0)\n",
    "            S_k = np.cov(X_k, rowvar=False)\n",
    "\n",
    "            m[k] = m_k\n",
    "            S[k] = S_k\n",
    "            posto[k] = np.linalg.matrix_rank(S_k)\n",
    "\n",
    "            iS_k = np.linalg.pinv(S_k)\n",
    "            sign, log_det_S_k = np.linalg.slogdet(S_k)\n",
    "            log_det_S_k = log_det_S_k if sign > 0 else -np.inf\n",
    "\n",
    "            prior_k = X_k.shape[0] / Ntrn\n",
    "            inv_covs[k] = iS_k\n",
    "            log_dets[k] = log_det_S_k\n",
    "            log_priors[k] = np.log(prior_k)\n",
    "\n",
    "        # Teste: Classificação das amostras\n",
    "        discriminant_scores = np.zeros((Ntst, len(unique_classes_in_train)))\n",
    "\n",
    "        for i, k in enumerate(unique_classes_in_train):\n",
    "            diff = X_test - m[k]\n",
    "            mahalanobis_dist = np.sum((diff @ inv_covs[k]) * diff, axis=1)\n",
    "            discriminant_scores[:, i] = mahalanobis_dist + log_dets[k] - 2 * log_priors[k]\n",
    "\n",
    "        predicted_class_indices = np.argmin(discriminant_scores, axis=1)\n",
    "        predicted_labels = unique_classes_in_train[predicted_class_indices]\n",
    "\n",
    "        correct = np.sum(predicted_labels == y_test)\n",
    "        TX_OK[r] = 100 * correct / Ntst\n",
    "\n",
    "    # Consolidação dos resultados\n",
    "    STATS = np.array([\n",
    "        np.mean(TX_OK), np.min(TX_OK), np.max(TX_OK), np.median(TX_OK), np.std(TX_OK)\n",
    "    ])\n",
    "\n",
    "    return STATS, TX_OK, X, m, S, posto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7e6c7",
   "metadata": {},
   "source": [
    "## Variante 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "963a6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variante1(data, Nr, Ptrain, reg_lambda):\n",
    "    \"\"\"\n",
    "    Implementa o classificador QDA supostamente com regularização de Tikhonov.\n",
    "\n",
    "    Parâmetros:\n",
    "    - data: Array numpy com os dados.\n",
    "    - Nr: Número de execuções.\n",
    "    - Ptrain: Percentual de dados para treinamento.\n",
    "    - alpha: Parâmetro de regularização (não utilizado no código).\n",
    "\n",
    "    Retorna:\n",
    "    - Mesmos outputs da função `quadratico`.\n",
    "    \"\"\"\n",
    "    n_total_samples, p_plus_1 = data.shape\n",
    "    num_features = p_plus_1 - 1\n",
    "    X_data = data[:, :-1]\n",
    "    y_data = data[:, -1].astype(int)\n",
    "\n",
    "    Ntrn = round(Ptrain * n_total_samples / 100)\n",
    "    Ntst = n_total_samples - Ntrn\n",
    "\n",
    "    K = np.max(y_data)\n",
    "    print(f'O problema tem {K} classes, {num_features} features. Lambda={reg_lambda}')\n",
    "\n",
    "    TX_OK = np.zeros(Nr)\n",
    "\n",
    "    for r in range(Nr):\n",
    "        permutation = np.random.permutation(n_total_samples)\n",
    "        train_indices, test_indices = permutation[:Ntrn], permutation[Ntrn:]\n",
    "\n",
    "        X_train, y_train = X_data[train_indices], y_data[train_indices]\n",
    "        X_test, y_test = X_data[test_indices], y_data[test_indices]\n",
    "\n",
    "        X, m, S, posto = {}, {}, {}, {}\n",
    "        inv_covs, log_dets, log_priors = {}, {}, {}\n",
    "\n",
    "        unique_classes_in_train = np.unique(y_train)\n",
    "\n",
    "        # Treinamento\n",
    "        for k in unique_classes_in_train:\n",
    "            X_k = X_train[y_train == k]\n",
    "            X[k] = X_k\n",
    "\n",
    "            m_k = np.mean(X_k, axis=0)\n",
    "            S_k = np.cov(X_k, rowvar=False) + np.eye(num_features) * reg_lambda \n",
    "\n",
    "            m[k] = m_k\n",
    "            S[k] = S_k  # Regularização de Tikhonov \n",
    "            posto[k] = np.linalg.matrix_rank(S_k)\n",
    "\n",
    "            iS_k = np.linalg.pinv(S_k)\n",
    "            sign, log_det_S_k = np.linalg.slogdet(S_k)\n",
    "            log_det_S_k = log_det_S_k if sign > 0 else -np.inf\n",
    "            \n",
    "            prior_k = X_k.shape[0] / Ntrn\n",
    "            inv_covs[k] = iS_k\n",
    "            log_dets[k] = log_det_S_k\n",
    "            log_priors[k] = np.log(prior_k)\n",
    "\n",
    "        # Teste\n",
    "        discriminant_scores = np.zeros((Ntst, len(unique_classes_in_train)))\n",
    "\n",
    "        for i, k in enumerate(unique_classes_in_train):\n",
    "            diff = X_test - m[k]\n",
    "            mahalanobis_dist = np.sum((diff @ inv_covs[k]) * diff, axis=1)\n",
    "            discriminant_scores[:, i] = mahalanobis_dist + log_dets[k] - 2 * log_priors[k]\n",
    "\n",
    "        predicted_class_indices = np.argmin(discriminant_scores, axis=1)\n",
    "        predicted_labels = unique_classes_in_train[predicted_class_indices]\n",
    "\n",
    "        correct = np.sum(predicted_labels == y_test)\n",
    "        TX_OK[r] = 100 * correct / Ntst\n",
    "\n",
    "    # Consolidação\n",
    "    STATS = np.array([\n",
    "        np.mean(TX_OK), np.min(TX_OK), np.max(TX_OK), np.median(TX_OK), np.std(TX_OK)\n",
    "    ])\n",
    "\n",
    "    return STATS, TX_OK, X, m, S, posto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd37c15e",
   "metadata": {},
   "source": [
    "## Variante 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "686a4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variante2(data, Nr, Ptrain):\n",
    "    \"\"\"\n",
    "    Implementa o classificador QDA com matriz de covariância POOLED.\n",
    "    Utiliza uma única matriz de covariância (pooled) para todas as classes.\n",
    "\n",
    "    Retorna:\n",
    "    - S: A matriz de covariância POOLED única (da última execução).\n",
    "    - posto: O rank da matriz POOLED (da última execução).\n",
    "    - Outros outputs são análogos à função `quadratico`.\n",
    "    \"\"\"\n",
    "    n_total_samples, p_plus_1 = data.shape\n",
    "    num_features = p_plus_1 - 1\n",
    "    X_data = data[:, :-1]\n",
    "    y_data = data[:, -1].astype(int)\n",
    "\n",
    "    Ntrn = round(Ptrain * n_total_samples / 100)\n",
    "    Ntst = n_total_samples - Ntrn\n",
    "\n",
    "    K = np.max(y_data)\n",
    "    print(f'O problema tem {K} classes, {num_features} features. Método LDA (Pooled Cov).')\n",
    "\n",
    "    TX_OK = np.zeros(Nr)\n",
    "    X_final, m_final, S_final, posto_final = {}, {}, None, None\n",
    "\n",
    "    for r in range(Nr):\n",
    "        permutation = np.random.permutation(n_total_samples)\n",
    "        train_indices, test_indices = permutation[:Ntrn], permutation[Ntrn:]\n",
    "        X_train, y_train = X_data[train_indices], y_data[train_indices]\n",
    "        X_test, y_test = X_data[test_indices], y_data[test_indices]\n",
    "\n",
    "        m, log_priors, priors = {}, {}, {}\n",
    "        unique_classes_in_train = np.unique(y_train)\n",
    "\n",
    "        current_X = {}\n",
    "        individual_S = {}\n",
    "\n",
    "        # Passo A: Calcular médias e covariâncias individuais\n",
    "        for k in unique_classes_in_train:\n",
    "            X_k = X_train[y_train == k]\n",
    "            current_X[k] = X_k\n",
    "            m[k] = np.mean(X_k, axis=0)\n",
    "            individual_S[k] = np.cov(X_k, rowvar=False)\n",
    "            priors[k] = X_k.shape[0] / Ntrn\n",
    "            log_priors[k] = np.log(priors[k])\n",
    "\n",
    "        # Passo B: Calcular a matriz de covariância única (pooled)\n",
    "        C_pool = np.zeros((num_features, num_features))\n",
    "        for k in unique_classes_in_train:\n",
    "            prior_k = priors[k]\n",
    "            C_pool += prior_k * individual_S[k]\n",
    "\n",
    "        inv_C_pool = np.linalg.pinv(C_pool)\n",
    "\n",
    "        if r == Nr - 1:\n",
    "            X_final, m_final, S_final, posto_final = current_X, m, C_pool, np.linalg.matrix_rank(C_pool)\n",
    "\n",
    "        # Teste\n",
    "        dist_scores = np.zeros((Ntst, len(unique_classes_in_train)))\n",
    "        for i, k in enumerate(unique_classes_in_train):\n",
    "            diff = X_test - m[k]\n",
    "            mahalanobis_dist = np.sum((diff @ inv_C_pool) * diff, axis=1)\n",
    "            dist_scores[:, i] = mahalanobis_dist - 2 * log_priors[k]\n",
    "\n",
    "        predicted_class_indices = np.argmin(dist_scores, axis=1)\n",
    "        predicted_labels = unique_classes_in_train[predicted_class_indices]\n",
    "\n",
    "        correct = np.sum(predicted_labels == y_test)\n",
    "        TX_OK[r] = 100 * correct / Ntst\n",
    "\n",
    "    STATS = np.array([np.mean(TX_OK), np.min(TX_OK), np.max(TX_OK), np.median(TX_OK), np.std(TX_OK)])\n",
    "    return STATS, TX_OK, X_final, m_final, S_final, posto_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ea9bc",
   "metadata": {},
   "source": [
    "## Variante 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ec90aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variante3(data, Nr, Ptrain, lambda_reg):\n",
    "    \"\"\"\n",
    "    Implementa o classificador com Regularização de Friedman (RDA).\n",
    "    Cria uma matriz de covariância para cada classe interpolando entre a matriz\n",
    "    individual da classe e a matriz pooled geral.\n",
    "\n",
    "    Parâmetros:\n",
    "    - lambda_reg: Parâmetro de regularização (0 <= lambda <= 1).\n",
    "    - Outros parâmetros e retornos são análogos às funções anteriores.\n",
    "    \"\"\"\n",
    "    if not (0 <= lambda_reg <= 1):\n",
    "        raise ValueError(\"O parâmetro lambda_reg deve estar entre 0 e 1.\")\n",
    "\n",
    "    n_total_samples, p_plus_1 = data.shape\n",
    "    num_features = p_plus_1 - 1\n",
    "    X_data = data[:, :-1]\n",
    "    y_data = data[:, -1].astype(int)\n",
    "\n",
    "    Ntrn = round(Ptrain * n_total_samples / 100)\n",
    "    Ntst = n_total_samples - Ntrn\n",
    "\n",
    "    K = np.max(y_data)\n",
    "    print(f'O problema tem {K} classes, {num_features} features. Lambda={lambda_reg}')\n",
    "\n",
    "    TX_OK = np.zeros(Nr)\n",
    "    X_final, m_final, S_final, posto_final = {}, {}, {}, {}\n",
    "\n",
    "    for r in range(Nr):\n",
    "        permutation = np.random.permutation(n_total_samples)\n",
    "        train_indices, test_indices = permutation[:Ntrn], permutation[Ntrn:]\n",
    "        X_train, y_train = X_data[train_indices], y_data[train_indices]\n",
    "        X_test, y_test = X_data[test_indices], y_data[test_indices]\n",
    "\n",
    "        m, log_priors = {}, {}\n",
    "        inv_covs, log_dets = {}, {}\n",
    "        current_X, current_S_reg, current_posto_reg = {}, {}, {}\n",
    "        unique_classes_in_train = np.unique(y_train)\n",
    "\n",
    "        # Treinamento (Método de Friedman)\n",
    "        scatter_matrices = {}\n",
    "        num_samples_per_class = {}\n",
    "        for k in unique_classes_in_train:\n",
    "            X_k = X_train[y_train == k]\n",
    "            current_X[k] = X_k\n",
    "            N_k = X_k.shape[0]\n",
    "\n",
    "            m[k] = np.mean(X_k, axis=0)\n",
    "            log_priors[k] = np.log(N_k / Ntrn) if N_k > 0 else -np.inf\n",
    "            num_samples_per_class[k] = N_k\n",
    "\n",
    "            if N_k > 1:\n",
    "                scatter_matrices[k] = (N_k - 1) * np.cov(X_k, rowvar=False)\n",
    "            else:\n",
    "                scatter_matrices[k] = np.zeros((num_features, num_features))\n",
    "\n",
    "        scatter_S_pool = Ntrn * np.cov(X_train, rowvar=False)\n",
    "\n",
    "        for k in unique_classes_in_train:\n",
    "            N_k = num_samples_per_class.get(k, 0)\n",
    "            S_k = scatter_matrices.get(k, np.zeros((num_features, num_features)))\n",
    "\n",
    "            numerator = (1 - lambda_reg) * S_k + lambda_reg * scatter_S_pool\n",
    "            denominator = (1 - lambda_reg) * N_k + lambda_reg * Ntrn\n",
    "\n",
    "            if denominator <= 0:\n",
    "                C_lambda_k = np.zeros((num_features, num_features))\n",
    "            else:\n",
    "                C_lambda_k = numerator / denominator\n",
    "\n",
    "            current_S_reg[k] = C_lambda_k\n",
    "            current_posto_reg[k] = np.linalg.matrix_rank(C_lambda_k)\n",
    "            inv_covs[k] = np.linalg.pinv(C_lambda_k)\n",
    "            sign, log_det = np.linalg.slogdet(C_lambda_k)\n",
    "            log_dets[k] = log_det if sign > 0 else -np.inf\n",
    "\n",
    "        if r == Nr - 1:\n",
    "            X_final, m_final, S_final, posto_final = current_X, m, current_S_reg, current_posto_reg\n",
    "\n",
    "        # Teste\n",
    "        discriminant_scores = np.zeros((Ntst, len(unique_classes_in_train)))\n",
    "        for i, k in enumerate(unique_classes_in_train):\n",
    "            if k in m:\n",
    "                diff = X_test - m[k]\n",
    "                mahalanobis_dist = np.sum((diff @ inv_covs[k]) * diff, axis=1)\n",
    "                discriminant_scores[:, i] = mahalanobis_dist + log_dets[k] - 2 * log_priors[k]\n",
    "            else:\n",
    "                discriminant_scores[:, i] = np.inf\n",
    "\n",
    "        predicted_class_indices = np.argmin(discriminant_scores, axis=1)\n",
    "        predicted_labels = unique_classes_in_train[predicted_class_indices]\n",
    "        correct = np.sum(predicted_labels == y_test)\n",
    "        TX_OK[r] = 100 * correct / Ntst\n",
    "\n",
    "    STATS = np.array([np.mean(TX_OK), np.min(TX_OK), np.max(TX_OK), np.median(TX_OK), np.std(TX_OK)])\n",
    "    return STATS, TX_OK, X_final, m_final, S_final, posto_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b77602",
   "metadata": {},
   "source": [
    "## Variante 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97779bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variante4(data, Nr, Ptrain):\n",
    "    \"\"\"\n",
    "    Implementa o classificador Naive Bayes Gaussiano.\n",
    "    Assume que os atributos são condicionalmente independentes, forçando a\n",
    "    matriz de covariância de cada classe a ser diagonal.\n",
    "\n",
    "    Retorna:\n",
    "    - S: Dicionário com as matrizes de covariância DIAGONAIS (da última execução).\n",
    "    - Outros outputs são análogos às funções anteriores.\n",
    "    \"\"\"\n",
    "    n_total_samples, p_plus_1 = data.shape\n",
    "    num_features = p_plus_1 - 1\n",
    "    X_data = data[:, :-1]\n",
    "    y_data = data[:, -1].astype(int)\n",
    "\n",
    "    Ntrn = round(Ptrain * n_total_samples / 100)\n",
    "    Ntst = n_total_samples - Ntrn\n",
    "\n",
    "    K = np.max(y_data)\n",
    "    print(f'O problema tem {K} classes, {num_features} features. Método Naive Bayes.')\n",
    "\n",
    "    TX_OK = np.zeros(Nr)\n",
    "    X_final, m_final, S_final, posto_final = {}, {}, {}, {}\n",
    "\n",
    "    for r in range(Nr):\n",
    "        permutation = np.random.permutation(n_total_samples)\n",
    "        train_indices, test_indices = permutation[:Ntrn], permutation[Ntrn:]\n",
    "        X_train, y_train = X_data[train_indices], y_data[train_indices]\n",
    "        X_test, y_test = X_data[test_indices], y_data[test_indices]\n",
    "\n",
    "        m, inv_covs, log_dets, log_priors = {}, {}, {}, {}\n",
    "        current_X, current_S, current_posto = {}, {}, {}\n",
    "        unique_classes_in_train = np.unique(y_train)\n",
    "\n",
    "        # Treinamento\n",
    "        for k in unique_classes_in_train:\n",
    "            X_k = X_train[y_train == k]\n",
    "            current_X[k] = X_k\n",
    "            m_k = np.mean(X_k, axis=0)\n",
    "\n",
    "            # Força a matriz de covariância a ser diagonal\n",
    "            S_full = np.cov(X_k, rowvar=False)\n",
    "            S_k = np.diag(np.diag(S_full))\n",
    "            \n",
    "            m[k] = m_k\n",
    "            current_S[k] = S_k\n",
    "            current_posto[k] = np.linalg.matrix_rank(S_k)\n",
    "\n",
    "            iS_k = np.linalg.pinv(S_k)\n",
    "            sign, log_det_S_k = np.linalg.slogdet(S_k)\n",
    "            log_det_S_k = log_det_S_k if sign > 0 else -np.inf\n",
    "            \n",
    "            prior_k = X_k.shape[0] / Ntrn\n",
    "            inv_covs[k] = iS_k\n",
    "            log_dets[k] = log_det_S_k\n",
    "            log_priors[k] = np.log(prior_k)\n",
    "\n",
    "        if r == Nr - 1:\n",
    "            X_final, m_final, S_final, posto_final = current_X, m, current_S, current_posto\n",
    "\n",
    "        # Teste\n",
    "        discriminant_scores = np.zeros((Ntst, len(unique_classes_in_train)))\n",
    "        for i, k in enumerate(unique_classes_in_train):\n",
    "            diff = X_test - m[k]\n",
    "            mahalanobis_dist = np.sum((diff @ inv_covs[k]) * diff, axis=1)\n",
    "            discriminant_scores[:, i] = mahalanobis_dist + log_dets[k] - 2 * log_priors[k]\n",
    "\n",
    "        predicted_class_indices = np.argmin(discriminant_scores, axis=1)\n",
    "        predicted_labels = unique_classes_in_train[predicted_class_indices]\n",
    "\n",
    "        correct = np.sum(predicted_labels == y_test)\n",
    "        TX_OK[r] = 100 * correct / Ntst\n",
    "\n",
    "    STATS = np.array([np.mean(TX_OK), np.min(TX_OK), np.max(TX_OK), np.median(TX_OK), np.std(TX_OK)])\n",
    "    return STATS, TX_OK, X_final, m_final, S_final, posto_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c713f63",
   "metadata": {},
   "source": [
    "# Atividade 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf6e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando os classificadores na ordem especificada...\n",
      "O problema tem 15 classes, 400 features. Método QDA.\n"
     ]
    }
   ],
   "source": [
    "# --- Carregamento dos dados ---\n",
    "try:\n",
    "    D = np.loadtxt('recfaces400.dat')\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo 'recfaces400.dat' não encontrado.\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "# --- Parâmetros ---\n",
    "Nr = 10      # Número de rodadas\n",
    "Ptrain = 80  # Percentual de treino\n",
    "\n",
    "print(\"Executando os classificadores na ordem especificada...\")\n",
    "\n",
    "# --- Execução e medição de tempo (seguindo o modelo) ---\n",
    "\n",
    "# 1. Quadrático\n",
    "start_time = time.time()\n",
    "STATS_0, TX_OK_0, _, _, _, _ = quadratico(D, Nr, Ptrain)\n",
    "Tempo_0 = time.time() - start_time\n",
    "print(\"Finalizado: Quadrático\")\n",
    "\n",
    "# 2. Variante 1 Regularization method 1 (Tikhonov)\n",
    "start_time = time.time()\n",
    "STATS_1, TX_OK_1, _, _, _, _ = variante1(D, Nr, Ptrain, 0.01)\n",
    "Tempo_1 = time.time() - start_time\n",
    "print(\"Finalizado: Variante 1\")\n",
    "\n",
    "# 3. Variante 2 One common COV matrix (pooled)\n",
    "start_time = time.time()\n",
    "STATS_2, TX_OK_2, _, _, _, _ = variante2(D, Nr, Ptrain)\n",
    "Tempo_2 = time.time() - start_time\n",
    "print(\"Finalizado: Variante 2\")\n",
    "\n",
    "# 4. Variante 3 Regularization method 2 (Friedman)\n",
    "start_time = time.time()\n",
    "STATS_3, TX_OK_3, _, _, _, _ = variante3(D, Nr, Ptrain, 0.5)\n",
    "Tempo_3 = time.time() - start_time\n",
    "print(\"Finalizado: Variante 3\")\n",
    "\n",
    "# 5. Variante 4 % Naive Bayes Local (Based on quadratico)\n",
    "start_time = time.time()\n",
    "STATS_4, TX_OK_4, _, _, _, _ = variante4(D, Nr, Ptrain)\n",
    "Tempo_4 = time.time() - start_time\n",
    "print(\"Finalizado: Variante 4\")\n",
    "\n",
    "# 6. Máxima Correlação (MaxCorr)\n",
    "start_time = time.time()\n",
    "STATS_MaxCorr, TX_OK_MaxCorr, _, _ = MaxCorr(D, Nr, Ptrain)\n",
    "Tempo_MaxCorr = time.time() - start_time\n",
    "print(\"Finalizado: MaxCorr\")\n",
    "\n",
    "# 7. Distância Mínima do Centroide (DMC)\n",
    "start_time = time.time()\n",
    "STATS_DMC, TX_OK_DMC, _, _ = DMC(D, Nr, Ptrain)\n",
    "Tempo_DMC = time.time() - start_time\n",
    "print(\"Finalizado: DMC\")\n",
    "\n",
    "# 8. 1-Vizinho Mais Próximo (1-NN)\n",
    "start_time = time.time()\n",
    "STATS_1NN, TX_OK_1NN, _, _ = NN1(D, Nr, Ptrain)\n",
    "Tempo_1NN = time.time() - start_time\n",
    "print(\"Finalizado: 1-NN\")\n",
    "\n",
    "print(\"\\nExecução de todos os classificadores finalizada.\")\n",
    "\n",
    "\n",
    "# --- Agrupamento dos resultados para exibição ---\n",
    "# A ordem aqui deve corresponder à ordem de execução\n",
    "classifier_names = [\n",
    "    'Quadrático',\n",
    "    'Variante 1',\n",
    "    'Variante 2',\n",
    "    'Variante 3',\n",
    "    'Variante 4',\n",
    "    'MaxCorr',\n",
    "    'DMC',\n",
    "    '1-NN'\n",
    "]\n",
    "\n",
    "all_stats = [\n",
    "    STATS_0,\n",
    "    STATS_1,\n",
    "    STATS_2,\n",
    "    STATS_3,\n",
    "    STATS_4,\n",
    "    STATS_MaxCorr,\n",
    "    STATS_DMC,\n",
    "    STATS_1NN\n",
    "]\n",
    "\n",
    "all_times = [\n",
    "    Tempo_0,\n",
    "    Tempo_1,\n",
    "    Tempo_2,\n",
    "    Tempo_3,\n",
    "    Tempo_4,\n",
    "    Tempo_MaxCorr,\n",
    "    Tempo_DMC,\n",
    "    Tempo_1NN\n",
    "]\n",
    "\n",
    "all_tx_ok = [\n",
    "    TX_OK_0,\n",
    "    TX_OK_1,\n",
    "    TX_OK_2,\n",
    "    TX_OK_3,\n",
    "    TX_OK_4,\n",
    "    TX_OK_MaxCorr,\n",
    "    TX_OK_DMC,\n",
    "    TX_OK_1NN\n",
    "]\n",
    "\n",
    "# --- Montagem e exibição do DataFrame ---\n",
    "stats_array = np.array(all_stats)\n",
    "data_for_df = {\n",
    "    'Média': stats_array[:, 0],\n",
    "    'Mínimo': stats_array[:, 1],\n",
    "    'Máximo': stats_array[:, 2],\n",
    "    'Mediana': stats_array[:, 3],\n",
    "    'Desvio Padrão': stats_array[:, 4],\n",
    "    'Tempo de execução (s)': all_times\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(data_for_df, index=classifier_names)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "print(\"\\n--- TABELA DE RESULTADOS (ORDEM ESPECIFICADA) ---\")\n",
    "display(df_results)\n",
    "\n",
    "\n",
    "# --- Plotagem do Boxplot ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.boxplot(all_tx_ok, patch_artist=False, labels=classifier_names) # patch_artist=False para estilo mais limpo\n",
    "plt.title('Comparação de Desempenho dos Classificadores (Dataset recfaces400)', fontsize=16)\n",
    "plt.xlabel('Classificador', fontsize=12)\n",
    "plt.ylabel('Taxa de Acerto (%)', fontsize=12)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carregamento dos dados ---\n",
    "try:\n",
    "    D = np.loadtxt('recfaces900.dat')\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo 'recfaces900.dat' não encontrado.\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "# --- Parâmetros ---\n",
    "Nr = 50      # Número de rodadas\n",
    "Ptrain = 80  # Percentual de treino\n",
    "\n",
    "print(\"Executando os classificadores na ordem especificada...\")\n",
    "\n",
    "# --- Execução e medição de tempo (seguindo o modelo) ---\n",
    "\n",
    "# 1. Quadrático\n",
    "start_time = time.time()\n",
    "STATS_0, TX_OK_0, _, _, _, _ = quadratico(D, Nr, Ptrain)\n",
    "Tempo_0 = time.time() - start_time\n",
    "print(\"Finalizado: Quadrático\")\n",
    "\n",
    "# 2. Variante 1 Regularization method 1 (Tikhonov)\n",
    "start_time = time.time()\n",
    "STATS_1, TX_OK_1, _, _, _, _ = variante1(D, Nr, Ptrain, 0.01)\n",
    "Tempo_1 = time.time() - start_time\n",
    "print(\"Finalizado: Variante 1\")\n",
    "\n",
    "# 3. Variante 2 One common COV matrix (pooled)\n",
    "start_time = time.time()\n",
    "STATS_2, TX_OK_2, _, _, _, _ = variante2(D, Nr, Ptrain)\n",
    "Tempo_2 = time.time() - start_time\n",
    "print(\"Finalizado: Variante 2\")\n",
    "\n",
    "# 4. Variante 3 Regularization method 2 (Friedman)\n",
    "start_time = time.time()\n",
    "STATS_3, TX_OK_3, _, _, _, _ = variante3(D, Nr, Ptrain, 0.5)\n",
    "Tempo_3 = time.time() - start_time\n",
    "print(\"Finalizado: Variante 3\")\n",
    "\n",
    "# 5. Variante 4 % Naive Bayes Local (Based on quadratico)\n",
    "start_time = time.time()\n",
    "STATS_4, TX_OK_4, _, _, _, _ = variante4(D, Nr, Ptrain)\n",
    "Tempo_4 = time.time() - start_time\n",
    "print(\"Finalizado: Variante 4\")\n",
    "\n",
    "# 6. Máxima Correlação (MaxCorr)\n",
    "start_time = time.time()\n",
    "STATS_MaxCorr, TX_OK_MaxCorr, _, _ = MaxCorr(D, Nr, Ptrain)\n",
    "Tempo_MaxCorr = time.time() - start_time\n",
    "print(\"Finalizado: MaxCorr\")\n",
    "\n",
    "# 7. Distância Mínima do Centroide (DMC)\n",
    "start_time = time.time()\n",
    "STATS_DMC, TX_OK_DMC, _, _ = DMC(D, Nr, Ptrain)\n",
    "Tempo_DMC = time.time() - start_time\n",
    "print(\"Finalizado: DMC\")\n",
    "\n",
    "# 8. 1-Vizinho Mais Próximo (1-NN)\n",
    "start_time = time.time()\n",
    "STATS_1NN, TX_OK_1NN, _, _ = NN1(D, Nr, Ptrain)\n",
    "Tempo_1NN = time.time() - start_time\n",
    "print(\"Finalizado: 1-NN\")\n",
    "\n",
    "print(\"\\nExecução de todos os classificadores finalizada.\")\n",
    "\n",
    "\n",
    "# --- Agrupamento dos resultados para exibição ---\n",
    "# A ordem aqui deve corresponder à ordem de execução\n",
    "classifier_names = [\n",
    "    'Quadrático',\n",
    "    'Variante 1',\n",
    "    'Variante 2',\n",
    "    'Variante 3',\n",
    "    'Variante 4',\n",
    "    'MaxCorr',\n",
    "    'DMC',\n",
    "    '1-NN'\n",
    "]\n",
    "\n",
    "all_stats = [\n",
    "    STATS_0,\n",
    "    STATS_1,\n",
    "    STATS_2,\n",
    "    STATS_3,\n",
    "    STATS_4,\n",
    "    STATS_MaxCorr,\n",
    "    STATS_DMC,\n",
    "    STATS_1NN\n",
    "]\n",
    "\n",
    "all_times = [\n",
    "    Tempo_0,\n",
    "    Tempo_1,\n",
    "    Tempo_2,\n",
    "    Tempo_3,\n",
    "    Tempo_4,\n",
    "    Tempo_MaxCorr,\n",
    "    Tempo_DMC,\n",
    "    Tempo_1NN\n",
    "]\n",
    "\n",
    "all_tx_ok = [\n",
    "    TX_OK_0,\n",
    "    TX_OK_1,\n",
    "    TX_OK_2,\n",
    "    TX_OK_3,\n",
    "    TX_OK_4,\n",
    "    TX_OK_MaxCorr,\n",
    "    TX_OK_DMC,\n",
    "    TX_OK_1NN\n",
    "]\n",
    "\n",
    "# --- Montagem e exibição do DataFrame ---\n",
    "stats_array = np.array(all_stats)\n",
    "data_for_df = {\n",
    "    'Média': stats_array[:, 0],\n",
    "    'Mínimo': stats_array[:, 1],\n",
    "    'Máximo': stats_array[:, 2],\n",
    "    'Mediana': stats_array[:, 3],\n",
    "    'Desvio Padrão': stats_array[:, 4],\n",
    "    'Tempo de execução (s)': all_times\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(data_for_df, index=classifier_names)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "print(\"\\n--- TABELA DE RESULTADOS (ORDEM ESPECIFICADA) ---\")\n",
    "display(df_results)\n",
    "\n",
    "\n",
    "# --- Plotagem do Boxplot ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.boxplot(all_tx_ok, patch_artist=False, labels=classifier_names) # patch_artist=False para estilo mais limpo\n",
    "plt.title('Comparação de Desempenho dos Classificadores (Dataset recfaces400)', fontsize=16)\n",
    "plt.xlabel('Classificador', fontsize=12)\n",
    "plt.ylabel('Taxa de Acerto (%)', fontsize=12)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
